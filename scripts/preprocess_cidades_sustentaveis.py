"""
Este script realiza o pr√©-processamento
da fonte de dados 'Cidades Sustent√°veis',
produzindo um arquivo CSV padronizado pronto para ingest√£o via dbt.

Etapas realizadas:
- Leitura e renomea√ß√£o de colunas relevantes
- Convers√£o segura de campo 'valor' para float
- Limpeza de quebras de linha e tipos
- Adi√ß√£o de data de processamento
- Upload para bucket GCS (bruto + processado)

PR√ìXIMOS PASSOS (fora do escopo deste script):

‚Üí No dbt:
  - Cria√ß√£o do modelo stg_indicadores__cidades_sustentaveis
  - Aplica√ß√£o de CASTs, filtros, testes de qualidade
    (not null, accepted_values, etc.)
  - Exclus√£o de registros inv√°lidos ou incompletos
  - Prepara√ß√£o do modelo dim_indicadores com enriquecimentos

‚Üí Recategoriza√ß√£o dos eixos:
  - O campo 'eixo' √© mantido como informado pela fonte
  - A categoriza√ß√£o final ser√° feita por:
      ‚Ä¢ IA supervisionada/classificador textual
      ‚Ä¢ ou join com tabela auxiliar (indicador_id ‚Üí eixo_padrao)
  - A normaliza√ß√£o permitir√° alinhar com o enum Eixos (SAUDE, EDUCACAO, etc.)

"""

import pandas as pd
from google.cloud import storage
import os
from datetime import datetime, timezone

# === CONFIGURA√á√ïES ===
CAMINHO_BRUTO = "dados/cidades-sustentaveis/indicadores.csv"
CAMINHO_LIMPO = "dados/cidades-sustentaveis/indicadores_padronizados.csv"
BUCKET_NAME = "observatudo-infra-www-data"
DESTINO_BRUTO = "indicadores/brutos/cidades-sustentaveis/indicadores.csv"
DESTINO_PROCESSADO = (
    "indicadores/processados/cidades-sustentaveis/indicadores.csv"
)

# === ETAPA 1: Leitura do CSV bruto ===
df = pd.read_csv(CAMINHO_BRUTO)

# === ETAPA 2: Renomear colunas relevantes ===
colunas_renomeadas = {
    "C√≥digo IBGE": "codigo_ibge",
    "Nome da cidade": "nome_cidade",
    "UF": "uf",
    "Estado Nome": "estado_nome",
    "Eixo": "eixo",
    "ID Indicador": "indicador_id",
    "Nome do indicador": "nome",
    "Formula do indicador": "formula",
    "Meta ODS": "meta_ods",
    "N√∫mero ODS": "numero_ods",
    "Nome do ODS": "nome_ods",
    "Descri√ß√£o do indicador": "descricao",
    "Ano de Preenchimento": "ano",
    "Valor": "valor",
    "Justificativa": "justificativa"
}

df = df.rename(columns=colunas_renomeadas)
df = df[list(colunas_renomeadas.values())]

# === ETAPA 3: Convers√µes seguras ===

# ‚Üí valor: texto com v√≠rgula ‚Üí float (ou NaN se n√£o for n√∫mero)
df["valor_original"] = df["valor"]  # backup
df["valor"] = pd.to_numeric(
    df["valor"]
    .astype(str)
    .str.replace(".", "", regex=False)
    .str.replace(",", ".", regex=False),
    errors="coerce"
)

# ‚Üí justificativa: remover quebras de linha
df["justificativa"] = df["justificativa"].fillna("").str.replace("\n", " ")

# ‚Üí ano: transformar em inteiro seguro
df["ano"] = pd.to_numeric(df["ano"], errors="coerce").astype("Int64")

# ‚Üí data de processamento
df["data_processamento"] = datetime.now(timezone.utc)

# === ETAPA 4: Estat√≠sticas de qualidade ===
total = len(df)
com_valor = df["valor"].notna().sum()
sem_valor = df["valor"].isna().sum()

print("üìä Estat√≠sticas do pr√©-processamento:")
print(f" - Total de registros: {total}")
print(f" - Registros com valor num√©rico: {com_valor}")
print(f" - Registros ignorados por valor inv√°lido: {sem_valor}")

# === ETAPA 5: Salvar CSV limpo localmente ===
os.makedirs(os.path.dirname(CAMINHO_LIMPO), exist_ok=True)
df.to_csv(CAMINHO_LIMPO, index=False)

# === ETAPA 6: Upload dos arquivos para o bucket ===
client = storage.Client()
bucket = client.bucket(BUCKET_NAME)

# Upload do bruto
blob_bruto = bucket.blob(DESTINO_BRUTO)
blob_bruto.upload_from_filename(CAMINHO_BRUTO)

# Upload do processado
blob_proc = bucket.blob(DESTINO_PROCESSADO)
blob_proc.upload_from_filename(CAMINHO_LIMPO)

print("‚úÖ Upload conclu√≠do com sucesso:")
print(f" - Bruto: {DESTINO_BRUTO}")
print(f" - Processado: {DESTINO_PROCESSADO}")
