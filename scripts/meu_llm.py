"""
üîç meu_llm.py

M√≥dulo auxiliar para comunica√ß√£o com o servidor Ollama local,
respons√°vel por classificar indicadores em eixos tem√°ticos como
Sa√∫de, Educa√ß√£o, Seguran√ßa, etc.

Este m√≥dulo:
- Valida a conectividade com o Ollama
- Verifica se o modelo desejado est√° carregado
- Fornece a fun√ß√£o `classificar_eixo(texto)` para uso no pr√©-processamento
- Pode ser usado isoladamente ou importado por outros scripts

Configura√ß√£o esperada no arquivo `.env`:

    OLLAMA_BASE_URL=http://localhost:11434
    OLLAMA_MODEL=qwen2.5-coder:0.5b
"""

import os
import requests
from dotenv import load_dotenv

load_dotenv()

OLLAMA_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
MODEL_NAME = os.getenv("OLLAMA_MODEL", "qwen2.5-coder:0.5b")


def check_server() -> bool:
    # Verifica se o servidor Ollama est√° acess√≠vel e o modelo est√° carregado
    try:
        r = requests.get(f"{OLLAMA_URL}/api/tags", timeout=3)
        r.raise_for_status()
        tags = r.json().get("models", [])
        available = any(m["name"] == MODEL_NAME for m in tags)
        if not available:
            print(f"‚ùå Modelo '{MODEL_NAME}' n√£o est√° dispon√≠vel no servidor Ollama.")
            return False
        print(f"‚úÖ Servidor acess√≠vel e modelo '{MODEL_NAME}' dispon√≠vel.")
        return True
    except Exception as e:
        print(f"‚ùå Falha ao conectar no servidor Ollama: {e}")
        return False


def classificar_eixo(texto: str) -> str:
    """
    Usa o modelo LLM local via Ollama para
    classificar um indicador em um eixo tem√°tico.
    Retorna o nome do eixo como string (ex: 'Sa√∫de', 'Governan√ßa', etc.)
    Em caso de erro, retorna 'Indefinido'.
    """
    prompt = (
        "Classifique o seguinte indicador em um dos eixos tem√°ticos: "
        "Sa√∫de, Educa√ß√£o, Assist√™ncia Social, Seguran√ßa, Meio Ambiente, Economia, Governan√ßa. "
        "Responda somente com o nome exato do eixo. \n\n"
        f"Indicador: {texto}"
    )

    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False
    }

    try:
        response = requests.post(f"{OLLAMA_URL}/api/generate", json=payload, timeout=10)
        response.raise_for_status()
        eixo = response.json().get("response", "").strip()
        return eixo or "Indefinido"
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao classificar indicador com LLM: {e}")
        return "Indefinido"

# Permite executar o script diretamente para testar o servidor


if __name__ == "__main__":
    if check_server():
        exemplo = "Acesso √† internet nas escolas dos ensinos fundamental e m√©dio"
        eixo = classificar_eixo(exemplo)
        print(f"\nüìå Exemplo de classifica√ß√£o:\n  Indicador: {exemplo}\n  ‚Üí Eixo IA: {eixo}")
    else:
        print("‚ö†Ô∏è Corrija a conex√£o com o servidor Ollama antes de continuar.")
